{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d15fcc6-dedf-4dae-9027-47a993a69cbd",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e7f044-5eb1-48ac-8163-de2595ee11d9",
   "metadata": {},
   "source": [
    "Principle Component Analysis (PCA) is a dimensionality reduction technique that aims to capture the maximum variance in the data using fewer principal components. The primary goal is to transform the original variables into a new set of variables, the principal components, which are orthogonal (uncorrelated), and which reflect the maximum variance in the data.\n",
    "\n",
    "### Mathematical Background\n",
    "\n",
    "Given a data matrix $X$ where each row represents a sample and each column represents a feature, the steps for PCA are:\n",
    "\n",
    "1. **Standardize the Data**: This step is crucial if the input features have different scales. We want each feature to have a mean of 0 and standard deviation of 1.\n",
    "\n",
    "2. **Compute the Covariance Matrix**: The covariance matrix, $S$, is computed as:\n",
    "\n",
    "   $$\n",
    "   S = \\frac{1}{n-1} X^T X\n",
    "   $$\n",
    "\n",
    "   where $n$ is the number of samples.\n",
    "\n",
    "3. **Eigen Decomposition**: Calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions of maximum variance (the principal components), and the eigenvalues represent the magnitude of the variance in each direction.\n",
    "\n",
    "4. **Sort Eigenvalues and Choose Principal Components**: Rank the eigenvectors based on the magnitude of their corresponding eigenvalues. The eigenvector with the highest eigenvalue is the first principal component, and so on.\n",
    "\n",
    "5. **Form the Projection Matrix**: Take the top $k$ eigenvectors (where $k$ is the number of dimensions you want to keep) and form a matrix $W$ with these eigenvectors.\n",
    "\n",
    "6. **Transform the Original Data**: Multiply the original data matrix, $X$, by the projection matrix, $W$:\n",
    "\n",
    "   $$\n",
    "   Y = XW\n",
    "   $$\n",
    "\n",
    "   This results in the data represented in terms of the principal components.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "PCA is essentially about finding the axes in the data that maximize variance. When data is projected onto these axes, it's transformed into a new coordinate system where the first axis corresponds to the first principal component that explains the most variance, the second axis corresponds to the second principal component that explains the second most, and so on.\n",
    "\n",
    "It's worth noting that while PCA reduces dimensionality, some information (or variance) is lost in the process. The amount of variance retained by each principal component is indicated by its eigenvalue.\n",
    "\n",
    "PCA is widely used in various domains, including image processing, genomics, finance, and many others, due to its efficiency and simplicity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea3d263-ce1f-485d-b276-bf7ced332220",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40954c51-033f-4235-ae6f-855a58b7a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b502648c-c5a4-494e-9128-da8a3a7c4051",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f56a21-8ab0-4cbc-a0ee-044a0a0a656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # reducing to 2 principal components\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plot the data in the new PCA space\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_pca[y == 0, 0], X_pca[y == 0, 1], color='red', label=data.target_names[0])\n",
    "plt.scatter(X_pca[y == 1, 0], X_pca[y == 1, 1], color='blue', label=data.target_names[1])\n",
    "plt.scatter(X_pca[y == 2, 0], X_pca[y == 2, 1], color='green', label=data.target_names[2])\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.legend()\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.show()\n",
    "\n",
    "# Display variance explained by each component\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eb4909-06c4-4d63-a03e-759f8e9e1da2",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "We first standardize the Iris dataset to have zero mean and unit variance for each feature.\n",
    "Then, we apply PCA to reduce the dataset's dimensionality from 4 features to 2 principal components.\n",
    "After applying PCA, we plot the data in the new PCA space.\n",
    "Finally, we display the variance explained by each of the principal components.\n",
    "You should see the Iris dataset plotted in terms of the first two principal components, with the data points color-coded based on their true class labels. The explained variance ratio will give you an idea of how much of the dataset's original variance is captured by each principal component."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
