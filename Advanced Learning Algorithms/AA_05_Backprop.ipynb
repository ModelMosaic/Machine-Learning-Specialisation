{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc5102d-0f1b-4bf4-b54d-ad747b7b0017",
   "metadata": {},
   "source": [
    "## 1. Theory Introduction to Backpropagation\n",
    "\n",
    "Backpropagation is the backbone of most of the modern neural networks. It's the algorithm used for minimizing the error by adjusting the weights of the network, based on the gradient of the loss function.\n",
    "\n",
    "### Basics:\n",
    "Given a feed-forward neural network, the initial pass where input is passed forward through the network to generate an output is known as **forward propagation**. The error or difference between this output and the true value is then calculated. \n",
    "\n",
    "Backpropagation, often known as \"backward propagation of errors,\" involves:\n",
    "1. Calculating the gradient of the loss function with respect to each weight by using the chain rule.\n",
    "2. Updating the weights in the network in the opposite direction to the gradient. This means if a particular weight was responsible for a large portion of the error, it would be adjusted more than a weight that was only responsible for a small portion of the error.\n",
    "\n",
    "A key thing to understand is that backpropagation requires a known, desired output for each input value â€“ it's a form of supervised learning.\n",
    "\n",
    "The basic idea revolves around how changing the weights impacts the overall error. The algorithm uses the chain rule of calculus to compute the error contribution of each weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b70079f-5ab6-4aad-a3b7-fcf520601588",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04f6c0-7772-4848-85ea-aaec00fd4ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "# Simple dataset (XOR problem)\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d7582f-dd09-47a0-8be0-254f90cb5e2f",
   "metadata": {},
   "source": [
    "## 3. Model coded in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60be588-13f2-4706-a149-c13fec6d1af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(0)\n",
    "weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "biases_hidden = np.random.rand(1, hidden_size)\n",
    "biases_output = np.random.rand(1, output_size)\n",
    "\n",
    "# Activation and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training the model with backpropagation\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation\n",
    "    hidden_layer_input = np.dot(X, weights_input_hidden) + biases_hidden\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + biases_output\n",
    "    predicted_output = sigmoid(output_layer_input)\n",
    "\n",
    "    # Calculate the error\n",
    "    error = y - predicted_output\n",
    "    \n",
    "    # Backpropagation\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "    \n",
    "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "    \n",
    "    # Update the weights\n",
    "    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
    "    weights_input_hidden += X.T.dot(d_hidden_layer) * learning_rate\n",
    "    \n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb3fc86-7a5b-4375-b3c1-56feb29ba221",
   "metadata": {},
   "source": [
    "## 4. Explanation\n",
    "\n",
    "In the provided Python code:\n",
    "\n",
    "1. **Forward Propagation**: We began by initializing random weights and biases. Inputs are passed through the network to get the predicted output.\n",
    "\n",
    "2. **Error Calculation**: The difference between the actual output (`y`) and the predicted output is calculated.\n",
    "\n",
    "3. **Backpropagation**:\n",
    "    * The error is multiplied with the derivative of the activation function (in this case, sigmoid) to get the gradient or 'direction and rate' at which the weights need to be changed.\n",
    "    * This process is then repeated for hidden layers (in this simple network, there's only one hidden layer).\n",
    "    \n",
    "4. **Weight Update**: The weights are updated in the direction which reduces the error. The `learning_rate` decides by how much we adjust the weights.\n",
    "\n",
    "After training for 10,000 epochs, our neural network should be able to approximate the XOR function. The printed `predicted_output` at the end of training should be close to the actual outputs of the XOR function for the given inputs.\n",
    "\n",
    "The essence of backpropagation is captured in how we calculate the error at each layer and adjust the weights accordingly. By repeatedly adjusting the weights using the gradients, we aim to minimize the error, allowing the network to learn the patterns in the data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
