{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc32d8ee-6228-4704-9b5c-a1b52c7d287c",
   "metadata": {},
   "source": [
    "## 1. Theory Introduction to Diagnosing Bias/Variance\n",
    "\n",
    "One of the critical challenges in machine learning is determining whether a model is suffering from bias or variance.\n",
    "\n",
    "- **Bias**: Refers to the error due to overly simplistic assumptions in the learning algorithm, which can make it underfit the data. A high bias model oversimplifies the problem and performs poorly both on the training set and unseen data.\n",
    "\n",
    "- **Variance**: Refers to the error due to too much complexity in the learning algorithm, making the model overfit the data. A high variance model captures noise in the training data and performs well on the training set but poorly on unseen data.\n",
    "\n",
    "### Trade-off:\n",
    "In general, as you increase the complexity of your model, you will see a reduction in error due to lower bias but an increase in error due to higher variance. Achieving a balance between these two types of error is crucial for creating models that generalize well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed69781-2223-4c0d-9594-bc405b83bcf8",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee13f00-40ae-4295-9a78-1da1b90680d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4153d805-06c9-4551-8952-1782fdc47279",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9800821-b141-4b0b-ae5d-9aca1e2b98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c95f50-269b-4a3a-9a75-30ff782a4ffe",
   "metadata": {},
   "source": [
    "## 3. Model coded in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7889fe8f-008e-41e1-b446-f02b2b0f3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a decision tree classifier with varying depths\n",
    "tree_depths = [1, 5, 10, 15]\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for depth in tree_depths:\n",
    "    clf = DecisionTreeClassifier(max_depth=depth)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate accuracy on training and test set\n",
    "    train_accuracy = accuracy_score(y_train, clf.predict(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "    \n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ffc2c-2fdb-42ae-8f28-cc39a23079cf",
   "metadata": {},
   "source": [
    "## 4. Explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfddc022-3c2d-46ec-8637-3116477a7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training and test accuracies\n",
    "plt.plot(tree_depths, train_accuracies, '-o', label='Training Accuracy')\n",
    "plt.plot(tree_depths, test_accuracies, '-o', label='Test Accuracy')\n",
    "plt.xlabel('Depth of Decision Tree')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Bias vs. Variance Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc818cf-10ec-4f8b-b442-eac28e928ede",
   "metadata": {},
   "source": [
    "From the plot:\n",
    "\n",
    "1. **Low Depth (High Bias)**: At a low depth, both the training and test accuracies are low, indicating that the model is too simplistic and suffers from high bias.\n",
    "\n",
    "2. **High Depth (High Variance)**: At very high depths, the training accuracy is high, but the test accuracy is lower. This indicates that the model has become too complex and is overfitting to the training data, capturing its noise.\n",
    "\n",
    "3. **Optimal Point**: The optimal depth lies somewhere in the middle, where the difference between training and test accuracies is minimal while both accuracies are relatively high.\n",
    "\n",
    "This analysis demonstrates the bias-variance trade-off. Too simple a model, and it can't capture the underlying patterns. Too complex, and it starts to fit the noise in the training data, failing to generalize well to new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
