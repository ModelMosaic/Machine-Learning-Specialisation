{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7cb25d6-4b85-4783-b34b-64f35ad1f2af",
   "metadata": {},
   "source": [
    "## 1. Theory Introduction to Decision Trees\n",
    "\n",
    "Decision Trees are a type of supervised learning algorithms used for both regression and classification tasks. The primary goal of using a Decision Tree is to create a model that predicts the target variable by learning simple decision rules inferred from the features of the data.\n",
    "\n",
    "### Key Points:\n",
    "- **Nodes**: Split for the value of a certain attribute.\n",
    "- **Edges**: Outcome of a split to the next node.\n",
    "- **Root**: The node that performs the first split.\n",
    "- **Leaves**: Terminal nodes that predict the final outcome.\n",
    "\n",
    "Advantages of Decision Trees:\n",
    "1. **Interpretability**: Trees are easy to understand and interpret as they mimic human decision-making.\n",
    "2. **Minimal Data Preparation**: Trees often require very little data preparation, including not requiring normalization.\n",
    "3. **Handle Multi-output**: Trees can predict multiple target attributes.\n",
    "\n",
    "Disadvantages include a propensity to overfit, especially when the tree is deep.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c021700a-fdde-4344-9403-911d25920c60",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48763663-57bd-41ab-a18e-6310c48c3f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d5037-1eb1-4ca2-96ea-649b0f088d0b",
   "metadata": {},
   "source": [
    "## 2. Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37e0f1-c91a-490e-849f-955ad048f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1220561-368a-40fb-a009-f0e776740490",
   "metadata": {},
   "source": [
    "## 3. Model coded in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c10a1-5f31-481d-ab07-41e197e4630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train a decision tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Visualizing the decision tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(clf, filled=True, feature_names=data.feature_names, class_names=data.target_names.tolist())  # Here's the fix: .tolist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae933d7d-d517-47b6-ab21-31a01579a911",
   "metadata": {},
   "source": [
    "## 4. Explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96037132-fa70-4034-8fc5-d0ff096df5eb",
   "metadata": {},
   "source": [
    "The Decision Tree visualization gives insights into how the algorithm makes decisions:\n",
    "\n",
    "1. **Root Node**: The root node (topmost node) tests a particular feature and splits the data accordingly. In our example, the decision criterion is based on one of the iris flower features.\n",
    "\n",
    "2. **Internal Nodes**: As we move down the tree, there are further tests on other features. Each test divides the data into subsets that get further processed by descendant nodes.\n",
    "\n",
    "3. **Leaf Nodes**: These are the final nodes of the tree, where no further splitting occurs. Each leaf node assigns a class label, representing the final prediction of the algorithm.\n",
    "\n",
    "The depth, or how many layers of nodes the tree has, can control the complexity of the Decision Tree. A deeper tree might capture more information about the data but risks overfitting to the training data.\n",
    "\n",
    "It's evident from our visualization that the Decision Tree makes a series of decisions based on feature values to classify the iris flowers into one of the three classes. By observing the tree, one can understand the importance of different features and the decision-making process of the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
