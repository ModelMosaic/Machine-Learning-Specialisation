{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cc70529-add6-4d03-b795-9d5430ffbc15",
   "metadata": {},
   "source": [
    "# 1. Theory Introduction: Trees Ensemble\n",
    "\n",
    "## Ensemble Learning\n",
    "Ensemble methods use multiple learning models to obtain better predictive performance than could be obtained from any of the constituent learning models. The idea behind ensemble methods is that combining predictions from multiple models can often produce better results, especially if the models have different strengths and weaknesses.\n",
    "\n",
    "## Random Forests\n",
    "Random Forest is a popular ensemble learning method that can be used for both classification and regression tasks. It creates multiple decision trees during training and merges their outputs for prediction, which helps to overcome the overfitting problem and adds robustness to the model. Random forests use bootstrapped datasets to train each tree and random subsets of features to split on at each node, adding randomness to the tree-building process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18716869-7e6e-4f6f-bbdd-1b079c533d9c",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccac4fe-6a6e-4200-8183-39d6085f8142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d813940-564c-4ebb-8575-e0190c630f7a",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421e44e8-7c7a-4ec3-af04-8d70c18f8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974497e-e8ce-4e3f-80ff-93f7550ecb16",
   "metadata": {},
   "source": [
    "## 3. Model coded in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e67108a-3866-4c17-a2ad-b7c9b483bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Training the Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f59ae-4543-4b34-a560-32eba03ea2f1",
   "metadata": {},
   "source": [
    "## 4. Explanation\n",
    "\n",
    "### Why Random Forests?\n",
    "The power of Random Forests (and ensemble methods in general) lies in combining the predictions of several base estimators (in this case, decision trees), which individually might not be very accurate, to produce a more robust and accurate prediction.\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "- **Bootstrap sampling**: Random forests randomly sample the data with replacement, creating different datasets called bootstrap datasets. Each of these datasets is used to train a decision tree.\n",
    "\n",
    "- **Feature Randomness**: At each node split, a random subset of features is considered, adding another layer of randomness. This ensures the individual trees are de-correlated and overfitting is reduced.\n",
    "\n",
    "- **Voting/Averaging**: Once all trees are trained, for a classification task, the mode of the classes predicted by individual trees is returned. For regression, the average prediction of individual trees is used.\n",
    "\n",
    "This combination of randomness ensures that the model does not overfit to the noise in the training data, making Random Forests one of the most powerful off-the-shelf algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
