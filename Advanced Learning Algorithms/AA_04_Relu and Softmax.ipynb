{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4caf3fe-316a-4f75-87fa-3ff6281c3432",
   "metadata": {},
   "source": [
    "# Relu and SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6ff01a-57e2-4b39-a8cc-2c00f0442fb7",
   "metadata": {},
   "source": [
    "## 1. Theory Introduction to Softmax and ReLU\n",
    "\n",
    "### Softmax\n",
    "The Softmax function is an activation function that can be thought of as an extension of the logistic function to multiple dimensions. It is typically used in the output layer of a neural network for multi-class classification problems. The Softmax function takes a vector of arbitrary real-valued scores and squashes it to a vector of values between zero and one that sum up to one. It's represented as:\n",
    "\n",
    "$$\n",
    "\\sigma(\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\mathbf{z} \\) is the input vector to the softmax function.\n",
    "- \\( K \\) is the number of classes.\n",
    "- \\( j \\) is the element of the output vector.\n",
    "\n",
    "### ReLU (Rectified Linear Activation)\n",
    "ReLU stands for Rectified Linear Activation, which is a type of activation function that is widely used in convolutional neural networks and deep learning models. The mathematical expression for ReLU is:\n",
    "\n",
    "$$\n",
    "f(x) = max(0, x)\n",
    "$$\n",
    "\n",
    "The function returns \\( x \\) if \\( x \\) is greater than or equal to zero, and returns zero otherwise. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time. If you feed the output from a neuron through the ReLU function and the output is below zero, it will convert it to zero. Neurons with an output of zero are considered inactive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae912cbd-4988-4c1a-a1b4-98443844c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a46e6-bcb9-4587-9868-d172424d2095",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3055c9-d04f-4111-af39-27d20f42e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with 3 classes\n",
    "X, y = make_blobs(n_samples=1000, centers=3, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f610ceab-0937-460a-bde6-195b0ea332bf",
   "metadata": {},
   "source": [
    "## 3. Model coded in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460adce4-8cc3-4f7f-b4a3-0d5e645ed259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a neural network model with ReLU in the hidden layer\n",
    "model_relu = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_dim=2, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  \n",
    "])\n",
    "\n",
    "model_relu.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_relu = model_relu.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8704cf26-c461-4db9-8f07-d965c5060909",
   "metadata": {},
   "source": [
    "## 4. Comparison of ReLU and Softmax\n",
    "\n",
    "### ReLU:\n",
    "ReLU, or Rectified Linear Activation, is an element-wise function defined as \\( f(x) = max(0, x) \\). The function returns \\( x \\) if it's positive, otherwise it returns zero. It is applied element-wise, meaning that it processes each pixel from the input data separately. This activation function has been found to greatly accelerate the convergence of stochastic gradient descent as compared to sigmoid/tanh functions. It's widely used in many deep neural networks.\n",
    "\n",
    "### Softmax:\n",
    "Softmax converts a vector of numbers into a probability distribution, making it suitable for multi-class classification problems in the output layer. It will highlight the largest values and suppress values which are significantly below the maximum value. This helps in producing a clearer, bolder decision boundary in multi-class problems.\n",
    "\n",
    "To clarify, ReLU is commonly used in hidden layers to introduce non-linearity and to avoid the vanishing gradient problem. On the other hand, softmax is used in the output layer of multi-class classification problems. It's not a matter of choosing between them for the same layer, but rather knowing where each of them fits best in a neural network architecture.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
