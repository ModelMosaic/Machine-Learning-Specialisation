{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d740e4dd",
   "metadata": {},
   "source": [
    "# Overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac085169-a9d0-49c9-ae19-4ee469f6f6a5",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431e27f-55e5-475d-9a71-49577ccabb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f564c-8699-4bdf-99dc-a439be0b2828",
   "metadata": {},
   "source": [
    "## 1. Overfitting Explanation in Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd484148-b8f8-4e3a-a006-55b3f5a5b2c1",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "Overfitting is a phenomenon in machine learning and statistics where a model learns the training data too closely, including its noise and outliers, rather than generalizing from the underlying distribution. This often results in a model that performs exceptionally well on the training data but poorly on unseen data or test data.\n",
    "\n",
    "### Characteristics of Overfitting:\n",
    "\n",
    "1. **High Variance**: The model becomes overly complex with too many parameters. This often leads to a wavy or erratic curve in regression problems.\n",
    "2. **Low Bias**: The model captures the training data very accurately, often with almost no error.\n",
    "3. **Poor Generalization**: Despite having a low training error, the model will have a much higher test error.\n",
    "\n",
    "### Causes of Overfitting:\n",
    "\n",
    "- **Too many features or parameters** relative to the number of observations.\n",
    "- **Lack of regularization** which can prevent complexity by penalizing certain model parameters if they're likely to cause overfitting.\n",
    "- **Noisy data**: If the noise in the training data is learned as concepts by the model.\n",
    "- **Too few data points** to train on.\n",
    "\n",
    "### Prevention:\n",
    "\n",
    "1. **Regularization**: Techniques like L1 (Lasso) and L2 (Ridge) regularization can add penalty terms for complexity, reducing the risk of overfitting.\n",
    "2. **Cross-validation**: Helps in understanding how the model performs on unseen data.\n",
    "3. **Pruning**: For decision trees, pruning can remove branches that have little power in predicting target values.\n",
    "4. **Adding more data**: Sometimes, increasing the volume of data can help in reducing overfitting.\n",
    "\n",
    "In essence, the main goal is to strike a balance between bias (underfitting) and variance (overfitting) to create a model that generalizes well to new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca3e9e-890c-442f-8788-e49256a52e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "np.random.seed(0)\n",
    "n_samples = 30\n",
    "degrees = [1, 4, 15]  # Linear, some polynomial, and high degree polynomial\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = np.sin(2 * np.pi * X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "# Notice that I removed the np.newaxis here\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:, np.newaxis], y, test_size=0.2)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "for i, degree in enumerate(degrees):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X_train, y_train)  # And also removed np.newaxis here\n",
    "\n",
    "    # Evaluate the models using cross-validation\n",
    "    scores = pipeline.score(X_test, y_test)\n",
    "    \n",
    "    X_test_fit = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "    plt.plot(X_test_fit, pipeline.predict(X_test_fit), label=\"Model\")\n",
    "    plt.scatter(X_train, y_train, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(f\"Degree {degree}\\nTest Score = {scores:.2f}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fad8de-122e-4123-8c59-ead160418534",
   "metadata": {},
   "source": [
    "In this example, we fit polynomial regressions of degree 1 (linear), 4, and 15 to a dataset generated from a sine curve. The model with degree 15 clearly overfits the training data, capturing noise and showing a very wavy curve, while the model with degree 1 underfits. The model with degree 4 offers a balanced fit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
