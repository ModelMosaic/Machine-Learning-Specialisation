{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93177d3",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfb4ce8-4104-42cb-ab11-e2df2a409e59",
   "metadata": {},
   "source": [
    "The lessons emphasized the significance of adjusting data such that all features align within a comparable range. For those curious about the rationale behind this, refer to the 'details' section below. For others, the subsequent segment will guide you through the practical steps of feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26513ca1",
   "metadata": {},
   "source": [
    "## Objective\n",
    "You will:\n",
    "\n",
    "- Leverage the multi-variable techniques established in the prior session\n",
    "- Execute Gradient Descent on datasets enriched with multiple attributes\n",
    "- Investigate the influence of the learning rate (alpha) on the gradient descent process\n",
    "- Enhance the efficiency of gradient descent by implementing feature scaling through z-score normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989951c4",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47cb5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf38309d-03f7-4f19-9f4b-8b6672c128cb",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69fae4-c080-4b7f-8742-9c5c98de1344",
   "metadata": {},
   "source": [
    "- Feature scaling, which entails dividing each positive feature by its peak value. More broadly, one can adjust each feature using its minimum and maximum values as per the formula: (x-min)/(max-min) Both approaches normalize features to fall between -1 and 1. The first method is best suited for positive features and is straightforward, making it ideal for the examples in the lecture. In contrast, the latter method is versatile and applies to all kinds of features.\n",
    "- Mean normalization: $x_i := \\dfrac{x_i - \\mu_i}{max - min} $ \n",
    "- Z-score normalization, which we will delve into in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84167b0-c39d-45d8-bbdf-93a4c43e557f",
   "metadata": {},
   "source": [
    "### z-score normalization \n",
    "\n",
    "Using the z-score normalization method, every feature will possess a zero mean and a standard deviation of one.\n",
    "\n",
    "To realize the z-score normalization, modify your input values following this equation:\n",
    "x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\tag{4}\n",
    "Here, the index $j$ identifies a feature or a column within the $\\mathbf{X}$ matrix. $Âµ_j$ represents the average of all values corresponding to feature (j), while $\\sigma_j$ denotes the standard deviation for feature (j).\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5}\\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{6}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note on Implementation: It's vital, when you normalize features, to retain the normalization parameters - the mean and the standard deviation utilized in the calculations. After deducing the model's parameters, we typically aim to forecast the values of new, unseen data. For a novel x value (like living room dimensions or bedroom count), you must first apply normalization using the mean and standard deviation derived from your initial training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4deaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_with_zscore(X):\n",
    "    \"\"\"\n",
    "    Normalizes the dataset using z-score method for each column.\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Original data with m samples and n features.\n",
    "      \n",
    "    Returns:\n",
    "      X_normalized (ndarray (m,n)): Data after applying z-score normalization for each feature.\n",
    "      mean_values (ndarray (n,)): Mean value for each feature.\n",
    "      std_values (ndarray (n,)): Standard deviation for each feature.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the mean for each feature.\n",
    "    mean_values = np.mean(X, axis=0)           # mean_values will have shape (n,)\n",
    "    \n",
    "    # Calculate the standard deviation for each feature.\n",
    "    std_values  = np.std(X, axis=0)            # std_values will have shape (n,)\n",
    "    \n",
    "    # Subtract the feature mean and divide by its standard deviation for each feature value.\n",
    "    X_normalized = (X - mean_values) / std_values      \n",
    "\n",
    "    return X_normalized, mean_values, std_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
