{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca418bd",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe77851-04d0-4100-a8a5-647353305a08",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d477b-3d76-4bed-9378-b652bfd3c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067e1b99-1244-453c-a168-d000a1fab399",
   "metadata": {},
   "source": [
    "## 1. Regularization in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e64e40-59bf-4b41-b08d-e95e4aae4ee5",
   "metadata": {},
   "source": [
    "**Regularization** is a technique used in machine learning to prevent **overfitting**. Overfitting happens when a model learns not just the underlying pattern of the data but also its noise. As a result, the model excels with the training data but underperforms on unseen data (e.g., validation or test data).\n",
    "\n",
    "To counter this, regularization introduces a penalty to the loss function that the algorithm is optimizing. This penalty prevents the model from becoming overly complex, a common cause of overfitting. In essence, while the algorithm still aims to fit the data accurately, regularization ensures it doesn't fit \"too perfectly,\" maintaining a level of generality.\n",
    "\n",
    "There are various regularization techniques, with the most prevalent being:\n",
    "\n",
    "## L1 Regularization (Lasso Regression)\n",
    "This method introduces a penalty equal to the absolute value of the magnitude of coefficients. As a result, some coefficients can become zero, which effectively selects features.\n",
    "\n",
    "$$ L = \\text{Loss}(Data|Model) + \\alpha \\sum |w_i| $$\n",
    "\n",
    "## L2 Regularization (Ridge Regression)\n",
    "This technique adds a penalty proportional to the square of the magnitude of coefficients. While it restricts coefficients from becoming overly large and dominating the output, it doesn't necessarily nullify them.\n",
    "\n",
    "$$ L = \\text{Loss}(Data|Model) + \\alpha \\sum w_i^2 $$\n",
    "\n",
    "## Elastic Net\n",
    "Elastic Net is a hybrid approach, incorporating elements of both L1 and L2 regularization.\n",
    "\n",
    "The intensity of the regularization is managed by the parameter \\( \\alpha \\). An \\( \\alpha \\) value near 0 denotes minimal regularization. As \\( \\alpha \\) grows, so does the strength of the regularization, pushing coefficients closer to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb10c92-d749-41df-9542-85e7df69cf30",
   "metadata": {},
   "source": [
    "## 2. Example of Regularization in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa5d80-0931-48d3-bcd7-7d5e007a742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "np.random.seed(0)\n",
    "X = np.sort(np.random.rand(40, 1) * 10, axis=0)\n",
    "y = 2 * X.squeeze() + 1 + np.sin(X.squeeze()) * 10 + np.random.randn(40) * 2\n",
    "\n",
    "X_train, X_test = X[:30], X[30:]\n",
    "y_train, y_test = y[:30], y[30:]\n",
    "\n",
    "# Linear regression without regularization\n",
    "linear_model = LinearRegression().fit(X_train, y_train)\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "\n",
    "# Linear regression with L1 regularization (Lasso)\n",
    "lasso_model = Lasso(alpha=1.0).fit(X_train, y_train)\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "\n",
    "# Linear regression with L2 regularization (Ridge)\n",
    "ridge_model = Ridge(alpha=1.0).fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='blue', s=10, label='Train data')\n",
    "plt.scatter(X_test, y_test, color='red', s=10, label='Test data')\n",
    "plt.plot(X_test, y_pred_linear, color='cyan', label='Linear Regression')\n",
    "plt.plot(X_test, y_pred_lasso, color='magenta', label='Lasso Regression')\n",
    "plt.plot(X_test, y_pred_ridge, color='green', label='Ridge Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Linear Regression MSE:\", mean_squared_error(y_test, y_pred_linear))\n",
    "print(\"Lasso Regression MSE:\", mean_squared_error(y_test, y_pred_lasso))\n",
    "print(\"Ridge Regression MSE:\", mean_squared_error(y_test, y_pred_ridge))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
