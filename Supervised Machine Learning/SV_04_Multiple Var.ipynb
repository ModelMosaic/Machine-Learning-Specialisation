{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5265242",
   "metadata": {},
   "source": [
    "# Multiple Variable Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8145a4",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Enhance our regression model functions to accommodate multiple features:\n",
    "- Modify data structures to handle various features.\n",
    "- Refactor prediction, cost, and gradient functions for multi-feature compatibility.\n",
    "- Leverage NumPy's np.dot for streamlined and faster vectorized implementations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7958717a",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e794921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a10ee",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3013803",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1800, 4, 2, 30], [1250, 3, 1, 28], [950, 2, 1, 20]])\n",
    "y_train = np.array([390, 215, 160])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794bdc92",
   "metadata": {},
   "source": [
    "## Matrix X containing our examples\n",
    "- $\\mathbf{x}^{(i)}$ represents the vector for the ith example, which can be expressed as $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n",
    "- The notation $x^{(i)}_j$ refers to the jth element within the ith example. Here, the superscript in parenthesis signifies the example's index, and the subscript indicates the element's position within that example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c4480",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of X: {X_train.shape}, Data type of X: {type(X_train)}\")\n",
    "print(X_train)\n",
    "print(f\"Shape of y: {y_train.shape}, Data type of y: {type(y_train)}\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf940f",
   "metadata": {},
   "source": [
    "## Parameter vector w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_init = 895.123456\n",
    "w_init = np.array([0.512345, 20.456789, -55.789012, -28.123456])\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c67db",
   "metadata": {},
   "source": [
    "## Multiple Variables\n",
    "The forecast in a model involving several variables follows the linear formula:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "Alternatively, employing vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "\n",
    "Here, $\\cdot$ represents a vector dot product.\n",
    "\n",
    "\n",
    "To illustrate the concept of the dot product, we'll craft predictions using both equations (1) and (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae732174",
   "metadata": {},
   "source": [
    "## Element-wise Prediction with Multiple Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187fd2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elementwise_prediction(x, w, b): \n",
    "    \"\"\"\n",
    "    Calculate prediction using linear regression with a loop.\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): A single example with shape (n,) representing multiple features.\n",
    "      w (ndarray): Model parameters with shape (n,).\n",
    "      b (scalar):  Bias parameter for the model.\n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  The calculated prediction.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    prediction = 0\n",
    "    for i in range(n):\n",
    "        partial_prediction = x[i] * w[i]\n",
    "        prediction += partial_prediction\n",
    "    prediction += b\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb8e11",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3.2\"></a>\n",
    "## Efficient Prediction Using Vector Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_prediction(x, w, b):\n",
    "    \"\"\"\n",
    "    Compute a prediction using linear regression in a vectorized manner.\n",
    "    \n",
    "    Parameters:\n",
    "      x (ndarray): An example with shape (n,) representing multiple features.\n",
    "      w (ndarray): Model parameters with shape (n,).\n",
    "      b (scalar):  Model bias parameter.\n",
    "\n",
    "    Returns:\n",
    "      p (scalar): The predicted value.\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4145ba",
   "metadata": {},
   "source": [
    "## Compute Cost With Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd9c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute the cost for linear regression.\n",
    "    \n",
    "    Parameters:\n",
    "      X (ndarray (m,n)): Matrix containing m examples, each with n features.\n",
    "      y (ndarray (m,)) : Vector containing target values for m examples.\n",
    "      w (ndarray (n,)) : Vector of model weights for n features.\n",
    "      b (scalar)       : Model bias term.\n",
    "\n",
    "    Returns:\n",
    "      cost (scalar): The computed cost.\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    total_error = 0.0\n",
    "    for i in range(m):\n",
    "        prediction = np.dot(X[i], w) + b\n",
    "        total_error += (prediction - y[i])**2\n",
    "    cost = total_error / (2 * m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac1e768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display the cost using our selected optimal parameters.\n",
    "calculated_cost = calculate_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost with chosen optimal parameters: {calculated_cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66129934",
   "metadata": {},
   "source": [
    "## Gradient Calculation for Multivariate Linear Regression   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_calculation(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Calculates the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data matrix containing m examples each with n features\n",
    "      y (ndarray (m,)) : Vector containing target values\n",
    "      w (ndarray (n,)) : Vector containing model parameters  \n",
    "      b (scalar)       : Bias term for linear regression\n",
    "      \n",
    "    Returns:\n",
    "      w_gradient (ndarray (n,)): Gradient with respect to the weights w.\n",
    "      b_gradient (scalar):       Gradient with respect to the bias b.\n",
    "    \"\"\"\n",
    "    m, n = X.shape   #(number of samples, number of features)\n",
    "    w_gradient = np.zeros((n,))\n",
    "    b_gradient = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        prediction_error = (np.dot(X[i], w) + b) - y[i]   \n",
    "        for j in range(n):                         \n",
    "            w_gradient[j] += prediction_error * X[i, j]    \n",
    "        b_gradient += prediction_error                    \n",
    "    w_gradient /= m                                \n",
    "    b_gradient /= m                                \n",
    "        \n",
    "    return b_gradient, w_gradient"
   ]
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "15456"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
