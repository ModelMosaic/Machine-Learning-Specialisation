{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5948225",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "You will learn \n",
    "- The sigmoid function, commonly known as the logistic function.\n",
    "- The logistic regression, which employs the sigmoid function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b17007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f646188-2967-4a90-83c4-4be3511dbdad",
   "metadata": {},
   "source": [
    "Sigmoid (Logistic) Function Exploration\n",
    ">Using our linear regression model, $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b$, we aim to predict $y$ from $x$ for classification. To ensure predictions fall between 0 and 1, we employ the sigmoid function.\n",
    "\n",
    "Sigmoid Function Formula\n",
    "$g(z) = \\frac{1}{1+e^{-z}}\\tag{1}$\n",
    "\n",
    "Here, $z$ is the output of the linear regression model, which can be either scalar (for a single example) or a vector with $m$ values (for multiple examples). Our Python implementation should cater to both formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a797f1a-6f25-44a0-b85c-34f4551bb87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z.\n",
    "    \n",
    "    Args:\n",
    "        z (ndarray): A scalar or numpy array of any size.\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: sigmoid(z), same shape as z.\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80722d5-2b97-49be-81dd-78a9bf471104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate values\n",
    "z_values = np.linspace(-10, 10, 100)\n",
    "sigmoid_values = sigmoid(z_values)\n",
    "\n",
    "# Plot the sigmoid curve\n",
    "plt.plot(z_values, sigmoid_values)\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"sigmoid(z)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf71635-ad49-4d08-bdd2-b34e3e4d4b60",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression applies the sigmoid function to a linear regression model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) \\tag{2} $$ \n",
    "\n",
    "Where:\n",
    "$$ g(z) = \\frac{1}{1+e^{-z}}\\tag{3} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3bd0cf-d48f-4df1-bf98-d83a392f61d1",
   "metadata": {},
   "source": [
    "Here's a simple Python example to illustrate logistic regression using the LogisticRegression class from sklearn. In this example, we'll use the Iris dataset, but focus only on two classes for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002e34d9-ca04-41f5-af30-ab6c5906f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data[data.target != 2]  # Take only the first two classes\n",
    "y = data.target[data.target != 2]  # Take only the first two classes\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy of the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
