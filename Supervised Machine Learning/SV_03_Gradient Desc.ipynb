{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e701e9",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18237a",
   "metadata": {},
   "source": [
    "## Objective\n",
    "you will Streamline the optimization of w and b through gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02be7904",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a658a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc7716-92c3-42ab-abf8-7aa45d3d3e66",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bbb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1.0, 2.0])   \n",
    "y_train = np.array([300.0, 500.0])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9896e3b",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.0.1\"></a>\n",
    "### Compute_Cost\n",
    "This was developed in the last lab. We'll need it again here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c40e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the cost\n",
    "def compute_cost(x, y, w, b):\n",
    "    m = len(x)\n",
    "    squared_errors = [(w * x[i] + b - y[i]) ** 2 for i in range(m)]\n",
    "    return sum(squared_errors) / (2 * m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3965f92-91a0-441b-b6b0-512714ceee10",
   "metadata": {},
   "source": [
    "## Gradient descent Overview\n",
    "Through this course, we've learned to predict using the linear model: $f_{w,b}(x^{(i)})$: $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "\n",
    "The goal is to fine-tune parameters w and b to minimize the error between our predictions and actual values. This error, known as the \"cost\" J(w,b): $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35879fcc-98c9-46be-9c64-71112a4146dd",
   "metadata": {},
   "source": [
    "Gradient descent process looks like:\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "In this process, we update the parameters w and b together. The gradients are computed as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "The term \"simultaneously\" implies calculating the gradients for all parameters first before making any updates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cae93ef",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.2\"></a>\n",
    "## Implement Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb1bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression.\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,)): m data examples.\n",
    "      y (ndarray (m,)): Corresponding target values.\n",
    "      w, b (scalar): Current model parameters.\n",
    "    \n",
    "    Returns:\n",
    "      dj_dw (scalar): Gradient of the cost with respect to w.\n",
    "      dj_db (scalar): Gradient of the cost with respect to b.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = x.shape[0]    \n",
    "    dj_dw, dj_db = 0, 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        prediction = w * x[i] + b\n",
    "        error = prediction - y[i]\n",
    "        dj_dw += error * x[i]\n",
    "        dj_db += error\n",
    "\n",
    "    return dj_dw / m, dj_db / m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a383b3",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a839d4",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.5\"></a>\n",
    "###  Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd396e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \"\"\"\n",
    "    Executes gradient descent to optimize w and b.\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): m data examples.\n",
    "      y (ndarray): Corresponding target values.\n",
    "      w_in, b_in (scalar): Initial model parameters.\n",
    "      alpha (float): Learning rate.\n",
    "      num_iters (int): Number of iterations for gradient descent.\n",
    "      cost_function: Function to compute cost.\n",
    "      gradient_function: Function to compute gradients.\n",
    "      \n",
    "    Returns:\n",
    "      w, b (scalar): Updated parameters after gradient descent.\n",
    "      J_history (list): Cost history.\n",
    "      p_history (list): Parameter history [w, b].\n",
    "    \"\"\"\n",
    "    \n",
    "    w, b = w_in, b_in\n",
    "    J_history, p_history = [], []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate and apply gradient updates\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b)\n",
    "        w -= alpha * dj_dw\n",
    "        b -= alpha * dj_db\n",
    "\n",
    "        # Store cost and parameter histories\n",
    "        if i < 100000:\n",
    "            J_history.append(cost_function(x, y, w, b))\n",
    "            p_history.append([w, b])\n",
    "        \n",
    "        # Periodically print cost and parameter info\n",
    "        if i % (num_iters // 10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e}, \"\n",
    "                  f\"dj_dw: {dj_dw:0.3e}, dj_db: {dj_db:0.3e}, \"\n",
    "                  f\"w: {w:0.3e}, b: {b:0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d2e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter initialization\n",
    "w_init, b_init = 0, 0\n",
    "\n",
    "# Gradient descent configurations\n",
    "iterations, learning_rate = 10000, 1.0e-2\n",
    "\n",
    "# Execute gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train, y_train, w_init, b_init, \n",
    "                                                   learning_rate, iterations, \n",
    "                                                   compute_cost, compute_gradient)\n",
    "\n",
    "print(f\"Parameters (w,b) obtained by gradient descent: ({w_final:.4f}, {b_final:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2de0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display cost versus iteration\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "\n",
    "# Plot start and end segments of the cost history\n",
    "ax1.plot(J_hist[:100])\n",
    "ax1.set_title(\"Cost vs. Iteration (Start)\")\n",
    "ax1.set_ylabel('Cost')\n",
    "ax1.set_xlabel('Iteration Step')\n",
    "\n",
    "ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\n",
    "ax2.set_title(\"Cost vs. Iteration (End)\")\n",
    "ax2.set_ylabel('Cost')\n",
    "ax2.set_xlabel('Iteration Step')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504a28b8",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "With the optimal parameters $w$ and $b$ in hand, you can employ the model to forecast housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d12b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"4000 sqft house prediction {w_final*4.0 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"5200 sqft house prediction {w_final*5.2 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"10000 sqft house prediction {w_final*10.0 + b_final:0.1f} Thousand dollars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02ecf8f",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.7.1\"></a>\n",
    "### Adjusting the Learning Rate\n",
    "\n",
    "During the lecture, we delved into the significance of the learning rate, $\\alpha$, as seen in equation(3). A bigger value of $\\alpha$ accelerates the convergence of gradient descent. However, if it's excessively large, gradient descent can deviate. In the previous section, we observed an example where the solution converged smoothly.\n",
    "\n",
    "Now, let's experiment by amplifying the value of $\\alpha$ and observe the outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe83ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial parameters\n",
    "initial_w = 0\n",
    "initial_b = 0\n",
    "# Define a high alpha value\n",
    "iteration_count = 10\n",
    "learning_rate = 8.0e-1\n",
    "# Execute gradient descent\n",
    "final_w, final_b, cost_history, param_history = gradient_descent(x_train, y_train, initial_w, initial_b, learning_rate, \n",
    "                                                                 iteration_count, compute_cost, compute_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6806e4e0",
   "metadata": {},
   "source": [
    "In the above scenario, both w and b oscillate between positive and negative values, with their magnitudes amplifying after each iteration. Additionally, with each iteration, the sign of $\\frac{\\partial J(w,b)}{\\partial w}$ flips, and the cost escalates instead of diminishing. This behavior is a distinct indicator that the learning rate has been set excessively high, leading to a divergent solution."
   ]
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "40291"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
