{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27d43788",
   "metadata": {},
   "source": [
    "# Enhancing Linear Models: Exploring Polynomial Regression and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300bc3de",
   "metadata": {},
   "source": [
    "## Objective\n",
    "- The realm of feature engineering to boost the predictive power of your models.\n",
    "- The intricacies of polynomial regression, enabling you to adapt linear regression techniques to capture complex, non-linear relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55449b23",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8650e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f44ac59",
   "metadata": {},
   "source": [
    "## Diving Deeper: Feature Engineering and the Power of Polynomial Regression\n",
    "\n",
    "At its core, linear regression equips us with tools to construct models like:\n",
    "$$f_{\\mathbf{w},b} = w_0x_0 + w_1x_1+ ... + w_{n-1}x_{n-1} + b \\tag{1}$$ \n",
    "\n",
    "When data deviates from a linear trend, as with housing prices influenced by varying home sizes, can our standard linear regression tools capture these curves? While we can adjust parameters $\\mathbf{w}$ and $\\mathbf{b}$ in (1) to fit our training data, this alone can't represent non-linear patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead72dce",
   "metadata": {},
   "source": [
    "## Polynomial Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305839d3",
   "metadata": {},
   "source": [
    "Polynomial features is a method used to add complexity to linear models by considering non-linear relationships of the input features. By adding polynomial terms (e.g., squared or cubed terms) as new features to our dataset, we can model non-linear relationships while still using a linear regression algorithm.\n",
    "\n",
    "For instance, if we have one feature x, then adding a polynomial feature would mean adding a term like \n",
    "x^2, etc., as new features.\n",
    "\n",
    "If we have a dataset with a feature x, a 2-degree polynomial feature would convert:\n",
    "x to: x, x^2\n",
    " \n",
    "A 3-degree polynomial would convert it to:\n",
    "x, x^2, x^3\n",
    " \n",
    "This concept can be extended to multiple features as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12638459-6399-408d-b72b-aa9c5894004f",
   "metadata": {},
   "source": [
    "## Example in Python\n",
    "\n",
    "Let's take a simple example using Python's scikit-learn library, which provides a convenient utility called PolynomialFeatures to generate these polynomial and interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f6316-0e25-4d6e-86f0-4ba15cb6cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Sample data\n",
    "X = np.array([\n",
    "    [2],\n",
    "    [3],\n",
    "    [4]\n",
    "])\n",
    "\n",
    "# Create polynomial features of degree 2\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "print(X_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf57da-cbe4-4755-a450-0d6a7538b1bd",
   "metadata": {},
   "source": [
    "## Selecting Features\n",
    "\n",
    "When modeling data, it might not be immediately evident which polynomial degree or which feature transformations would be most appropriate. Choosing the right feature set is critical for preventing underfitting (when the model is too simple to capture the underlying patterns) and overfitting (when the model is excessively complex and captures the noise in the data).\n",
    "\n",
    "Feature selection methods can help determine which features (or combinations of features) are most predictive. In the context of polynomial regression, the question might be about the correct polynomial degree to use or which polynomial terms to include.\n",
    "\n",
    "In the given problem, the equation y = w0*x0 + w1*x1^2 + w2*x2^3 + b suggests adding polynomial terms up to the third degree, but is this the optimal choice? Should we maybe consider even higher degrees, or perhaps some of these terms are unnecessary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c0bff-a46e-4241-a994-8b46908b7a58",
   "metadata": {},
   "source": [
    "## Python example\n",
    "Let's say we have some data, and we want to determine whether adding x^2 and x^3 terms improves the fit of our model. We'll use simple linear regression as our model for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89d7ce-b615-4e9f-a14d-13aadb5bd894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sample data\n",
    "x = np.linspace(0, 2 * np.pi, 100)\n",
    "y = np.sin(x) + 0.1 * np.random.randn(100)  # sine curve with some noise\n",
    "\n",
    "# Reshape x for the model training\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "# Using polynomial features\n",
    "degrees = [1, 2, 3]  # Linear, quadratic, and cubic\n",
    "errors = []\n",
    "\n",
    "for degree in degrees:\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    model = LinearRegression().fit(X_poly, y)\n",
    "    y_pred = model.predict(X_poly)\n",
    "    \n",
    "    plt.plot(x, y_pred, label=f'Degree {degree}')\n",
    "    errors.append(mean_squared_error(y, y_pred))\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label=\"True Values\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for degree, error in zip(degrees, errors):\n",
    "    print(f\"Degree {degree} MSE: {error:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
