{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6b299c2",
   "metadata": {},
   "source": [
    "# Logistic Regression, Logistic Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fae2af-7781-4df9-a150-e2ddef750587",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb4111-93e9-49e5-a279-e70ca118e4f2",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8734a3-540f-4467-be33-2f2ba0cdcfe7",
   "metadata": {},
   "source": [
    "## logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6b9d8-ee42-429b-8ae8-538e6eaa01a4",
   "metadata": {},
   "source": [
    "Logistic Regression is a statistical method for analyzing datasets in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes). It is used to predict a binary outcome (1 / 0, Yes / No, True / False) given a set of independent variables. To represent binary/categorical outcome, we use dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354ca47d-8ce2-44e4-9eb6-d1900b5b0558",
   "metadata": {},
   "source": [
    "- Sigmoid Function: The core idea of Logistic Regression is the utilization of the sigmoid function (also called the logistic function). It can map any value between 0 and 1, making it useful for a probability estimate.\n",
    "- Decision Boundary: Logistic regression produces a decision boundary that separates the classes. For a 2D dataset, this boundary is a line\n",
    "- Estimation: Logistic regression estimates the probability that a given instance belongs to a particular category.\n",
    "- Multiclass Classification: Though inherently binary, logistic regression can be adapted for multiclass classification using techniques like \"One vs All\" (OvA) or \"Softmax Regression\" for mutually exclusive classes.\n",
    "- Cost Function: The cost function used in logistic regression is log loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360831d-c2e4-4232-9af9-2119dcb4c61d",
   "metadata": {},
   "source": [
    "Now, let's illustrate Logistic Regression with a Python example using the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e87d8c-8faa-446b-b9e2-b8dc27477080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # We take only the first two features for simplicity\n",
    "y = (iris.target != 0) * 1  # Convert the target to binary\n",
    "\n",
    "# Instantiate and fit a Logistic Regression model\n",
    "logreg = LogisticRegression(C=1e5, solver='lbfgs')\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# Plotting the decision boundary\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "h = .02  # Step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(8, 6))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title('Logistic Regression on Iris Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4143b9-8de2-411f-aae9-e27a6770f7c8",
   "metadata": {},
   "source": [
    "## Logistic Loss Function (Log Loss)\n",
    "\n",
    "Logistic loss, also known as log loss or cross-entropy loss, is commonly used in classification problems. The purpose of the logistic loss function is to quantify the difference between the predicted probabilities for the actual class and the other classes.\n",
    "\n",
    "For a binary classification:\n",
    "\n",
    "- **Predicted Probability**: Let \\( \\hat{y} \\) be the predicted probability of an instance belonging to class 1.\n",
    "  \n",
    "- **Actual Label**: Let \\( y \\) be the actual label of the instance, where \\( y \\in \\{0,1\\} \\).\n",
    "\n",
    "The logistic loss for that instance is given by:\n",
    "\n",
    "\\[ \\mathcal{L}(\\hat{y}, y) = - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y}) \\]\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "- **When \\( y = 1 \\)**: The loss is \\( -\\log(\\hat{y}) \\). As \\( \\hat{y} \\) approaches 1, the loss goes to 0. Conversely, as \\( \\hat{y} \\) approaches 0, the loss goes to infinity.\n",
    "  \n",
    "- **When \\( y = 0 \\)**: The loss is \\( -\\log(1-\\hat{y}) \\). As \\( \\hat{y} \\) approaches 0, the loss goes to 0. Conversely, as \\( \\hat{y} \\) approaches 1, the loss goes to infinity.\n",
    "\n",
    "This implies that the more confident the incorrect predictions are, the higher the loss will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168fba6-7f57-44a9-a583-90ea633e2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the logistic loss.\n",
    "    \n",
    "    Args:\n",
    "        y_true (numpy array): Array of true binary labels.\n",
    "        y_pred (numpy array): Array of predicted probabilities.\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: Array of logistic loss values for each prediction.\n",
    "    \"\"\"\n",
    "    # Ensure no value is exactly 0 or 1 for stability\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    loss = - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
    "    return loss\n",
    "\n",
    "# Example usage:\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_pred = np.array([0.1, 0.9, 0.7, 0.3])\n",
    "\n",
    "print(\"Logistic Loss:\", logistic_loss(y_true, y_pred))\n",
    "\n",
    "# Visualizing the logistic loss for varying predicted probabilities\n",
    "y_pred_range = np.linspace(0, 1, 100)\n",
    "plt.plot(y_pred_range, logistic_loss(np.ones_like(y_pred_range), y_pred_range), label=\"Actual: 1\")\n",
    "plt.plot(y_pred_range, logistic_loss(np.zeros_like(y_pred_range), y_pred_range), label=\"Actual: 0\")\n",
    "plt.title(\"Logistic Loss vs. Predicted Probability\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
